{\rtf1\ansi\ansicpg1251\deff0\nouicompat\deflang1058{\fonttbl{\f0\fnil\fcharset0 Calibri;}}
{\*\generator Riched20 10.0.16299}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\f0\fs22\lang9 Another branch of statistical inference concerns something\par
called significance test.\par
Significance test works with hypotheses.\par
Hypotheses are statements on value of a population\par
quantity of interest.\par
So here, this is a very long definition\par
of what a hypothesis is.\par
What it means is that, for example, you\par
have a value in the population that you're interested in doing\par
inference upon.\par
Then hypothesis is a statement regarding that value.\par
It may not be true.\par
It may be true or may be incorrect.\par
But it is a statement that you're\par
trying to collect evidence-- collect information--\par
to prove or disprove.\par
It usually proposes a model for the population.\par
So as I have explained previously,\par
normal distributions are probability distribution models\par
for random phenomena.\par
So it is a model.\par
If you specify the mean of a normal distribution,\par
you actually specify a model for the population.\par
, So therefore very often when we out significance test,\par
we specify a value of interest for the population.\par
We actually propose a model for the population, which\par
we want to evaluate using data come from the true data\par
generation process from the population.\par
So here you have a statement propose a potential model\par
for the population.\par
On the other hand, you have the data generated\par
from the population, and you want\par
to check whether those two agree with each other.\par
The consistency between the observed data and the model\par
proposed in the statement is precisely\par
the thing we want to evaluate in the significance test.\par
This consistency will be used to decide whether the hypothesis\par
should be rejected or not.\par
So this-- if you have read state report from data analysis,\par
in particular a significance test,\par
you probably have seen a statement\par
such as, the null hypothesis was rejected at so and so level.\par
So this is what we mean.\par
That we decide whether the statement\par
is likely to be a true statement about population,\par
or is not likely to be a true statement about the population.\par
Now, let's consider example by imagine that we\par
have a coin in question.\par
I want to test whether this coin is biased or not.\par
So a coin is being checked for fairness.\par
We know that a fair coin produces\par
heads 50% percent of the time.\par
And to check that we cannot just ask the coin, saying,\par
are you 50%?\par
Are you a fair coin or not?\par
We need to collect, observe data.\par
The data we observe come from 10 tosses using this coin.\par
The court is actually biased so it produce heads 90% of time.\par
In the 10 tosses, the data we observe\par
is 10 heads observed for this coin.\par
If this coin is fair, the probability\par
of observing 10 heads is less than .001.\par
Actually, we had this as one of the [? access ?]\par
previously, so you know the process number here.\par
The coin is fair.\par
The probability of observing that is 1 over 1,024.\par
For this biased one however, the probability is actually 35%.\par
But you do not know that, because you're examining\par
this coin for fairness.\par
So what you have-- what you want to ask is,\par
if this coin is fair, that it produced heads 50% of the time,\par
and using 10 tosses you observe 10 heads,\par
and the chance of that happening if the coin is fair\par
is very small.\par
Then our question is whether this evidence--\par
we collected 10 heads for 10 tosses--\par
is a statistically significant evidence against the statement\par
that we want to check.\par
That is, the coin is fair.\par
So this is precisely the logic we\par
want to exercise in a significance test.\par
That we look at a statement, we look\par
at the model generated according to the statement,\par
and we check the consistency of the statement with data.\par
Like here, this consistency is usually\par
measured by a probability.\par
If the probability is very small, we ask the question,\par
is this probability small enough for us\par
to question the validity of the original statement\par
that we're checking.\par
So test a hypothesis formally, we first\par
need to state a hypothesis, and the hypothesis\par
propose a model for the sampling distribution of a statistic.\par
Data generates one value from that statistic,\par
then evidence against or supporting a hypothesis\par
should be a measure of distance between the model and the data.\par
The data generates one value, the model\par
represents a distribution.\par
So how can we measure a distance between a value\par
and the mathematical equation-- a mathematical function\par
representing the model?\par
To achieve that in statistics we use a test statistic, or a test\par
statistics to calculate the numerical distance.\par
However, the extent of evidence carried by the test statistic\par
are further evaluated p-values.\par
P-values is a probability.\par
It's defined as the probability assuming the null hypothesis is\par
true-- assuming the statement we claim about a population\par
value of interest is true.\par
Therefore, we have a model for the sampling distribution\par
of statistics.\par
It's a probability under this assumption of an event--\par
so it's a probability of event.\par
The definition of the event is, we\par
observe evidence carried by the sample statistic\par
against the null hypothesis.\par
For example, against the claim that the coin is\par
fair-- against the null hypothesis\par
the coin is fair-- and support alternative,\par
which is the alternative statement\par
we can have of a population.\par
For example, the coin is actually biased.\par
Against this claim that the coin is fair,\par
supporting the claim that the coin is biased.\par
At the level that is as strong as what is observed,\par
like 10 heads out of 10 tosses, which is very strong--\par
the strongest.\par
But for some case that we can expect to observe stronger if\par
the alternative is true.\par
So the p-value is a probability under the null hypothesis--\par
under the probability model that we can assume under the claim\par
that you have a value in the null hypothesis corresponding\par
to population value.\par
The probability concerns the event\par
that we observe evidence against null hypothesis,\par
supporting alternative.\par
So this is the entire definition of the event for which we're\par
evaluating the probability.\par
So the probability after evaluation\par
becomes p-value if this probability is very small.\par
Meaning that if the null hypothesis is truly\par
the model for the sampling variation of the statistic,\par
it's very unlikely for us to observe what we have observed.\par
So the p-value count concerns that-- whether it\par
is likely to observe what we have observed in the observed\par
data, if the null hypothesis proposed the true model\par
for the sample statistic.\par
We reject the null hypothesis when the p-value is\par
below certain level, and that level\par
is called significant level.\par
Now, back to the coin example.\par
That the null hypothesis we use is that we have a fair coin,\par
probability of a head equal to 50%.\par
Alternative hypothesis is a biased coin,\par
probability of head does not equal to 50%.\par
Data represent from 10 tosses we observe number of head\par
equal to 10, 100% head.\par
Evidence against the null hypothesis too many or too few\par
heads.\par
So this definition of evidence against null hypothesis\par
exists even without observing any data.\par
It's just we think if the null hypothesis is true,\par
you should have approximately equal number heads and tails.\par
If the alternative is true, you tend\par
to observe too many heads or too few heads.\par
So therefore, the evidence against the null hypothesis\par
is when you observe a departure of what expected\par
and the null hypothesis.\par
The distance for this particular data-- the distance\par
between the data and the expected under the null\par
hypothesis is 100% minus 50%.\par
We normalize this distance using standard error of this sample\par
proportion.\par
So standard error in statistic inference\par
is a measure computed from real data\par
for sampling variability of a statistic.\par
So here, the statistic we're looking at\par
is sample proportion.\par
And using statistical tools, we are\par
able to derive standard error, which\par
is an estimate of the variability of this proportion.\par
So we can normalize this distance using such quantity.\par
After doing that, we can compute p-values from some probability\par
distributions that we can derive under the null hypothesis.\par
The p-value tells how likely it is for us to observe a sample\par
proportion this far from the value stated\par
in the null hypothesis.\par
Or even further, even stronger.\par
Given this, we will be able to accept null hypothesis\par
if the p-value is large, reject null hypothesis\par
if the p-value is small.\par
So here the notation H0 in statistics\par
is for the null hypothesis.\par
H stands for the hypothesis, 0 stands\par
for null-- the null hypothesis.\par
So in practice, given real data, you\par
have the following four scenarios.\par
The unknown truths could be the null hypothesis\par
is actually true, or the null hypothesis is false.\par
So the coin is fair, or the coin is biased.\par
The conclusion from our significant test\par
could be you accept the statement\par
the null hypothesis is true, or the statement that the coin is\par
fair.\par
Or you reject the null hypothesis\par
after observing strong evidence.\par
After observing statistical significance\par
you reject the null hypothesis and conclude\par
that the coin is biased.\par
So in this combination of the truth and the conclusion,\par
we have four combinations.\par
So if the null hypothesis is true,\par
and you accept the null hypothesis, perfect.\par
We have made the right decision.\par
Or, if the null hypothesis is true, we actually reject it.\par
Then that will make an error.\par
That will make a wrong decision, because the null hypothesis is\par
true and you accept by chance,\par
observe a strong evidence against it.\par
Remember, the p-value describes the probability\par
of observing a strong evidence against the null hypothesis.\par
If the p-value is 5%, it means that even when\par
null hypothesis is true, you can still\par
have 5% chance of observing such a strong evidence.\par
So you still have a chance of rejecting the null hypothesis,\par
even though the probability is very small.\par
So you can still make this error.\par
This kind of error is called type one error.\par
If the null hypothesis is false but the data didn't\par
show enough evidence, you actually\par
accepted the null hypothesis.\par
That is also an error, because you should have concluded\par
that the null hypothesis is not true,\par
but the data didn't give you enough significant evidence,\par
and you failed to reject the null hypothesis.\par
So this is called type two error.\par
While if you have, in this scenario,\par
that the null hypothesis is not true\par
and the data showed enough evidence,\par
you reject null hypothesis.\par
This is another perfect outcome.\par
The chance of deriving this conclusion under this scenario\par
is described by the probability called power.\par
Which is, given random sample data out\par
of an alternative hypothesis-- out of a [? choose ?]\par
different from the null hypothesis--\par
what is the probability you actually\par
can reject the null hypothesis?\par
This is very important in designing a real data project,\par
because we want to control the sample size to give us\par
enough power to reject the null hypothesis when\par
the null hypothesis is actually not true.\par
The procedure we have used in constructing\par
the significant test is, we aim to have\par
as large power as possible while controlling for type one error.\par
So the process has a nominal type one error rate.\par
It's where we decide to cut the significant [? test-- ?]\par
where we decide to reject null hypothesis when the p-value is\par
below a certain threshold.\par
So that's how we control type one error.\par
After control type one error, we increase\par
the sample size and implement and other procedures\par
to have better power.\par
And it is also known that-- in this fourth scenario,\par
it's also known as true negative, false positive,\par
false negative, and true positive.\par
As in a lot of immersion learning discussions.\par
End of transcript. Skip to the start.\par
  Previous\par
}
 