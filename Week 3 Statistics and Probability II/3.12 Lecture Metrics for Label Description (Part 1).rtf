{\rtf1\ansi\ansicpg1251\deff0\nouicompat\deflang1058{\fonttbl{\f0\fnil\fcharset0 Calibri;}}
{\*\generator Riched20 10.0.16299}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\f0\fs22\lang9 Label descriptions are generated by metrics.\par
So in the case of what we've seen,\par
the metric that we have is I want\par
to maximize probability of the word.\par
And I take the word that maximizes probability.\par
So in this case, it might be "the" for group one\par
and then "and" for group one, et cetera, et cetera.\par
So any summary, or any automatically\par
generated summary is going to have a metric.\par
Probability, when you're ranking words by probability,\par
chooses words that are really, really common.\par
So in this case, it's words like "the," "and,"\par
et cetera, et cetera.\par
We tend to remove stop words, because aside\par
from causing problems with our statistical techniques,\par
they can also script summaries, such as "the," "and,"\par
et cetera, et cetera, where we'll basically just\par
be including really, really common words\par
in the English language.\par
When you use probability, you also\par
get a few uncommon words that tend\par
to align very well with labels.\par
So if I were looking through that Google News sheet,\par
I might have words like "Cavaliers" or "LeBron."\par
It's not words that come up often in speech.\par
But they would be coming up fairly often when it comes\par
to a topic about basketball.\par
And these strong point of these sorts of models\par
is that they tend to align very, very\par
well with underlying models.\par
So in the case of topic modeling, well,\par
we're given this probability vector of words for each topic.\par
So aligning it by decreasing probability\par
is a really good description of what's in that model.\par
It might not be great for human exploration, where you're using\par
that topic model as a guide.\par
Secondly, it does tend to choose words that are really\par
common-- words that you and I and probably\par
every three-year-old would know, maybe not Cavaliers,\par
but words like "the" and "and."\par
So it won't go through and pick out words that only occur once\par
or twice or those $10-words that you get in your "Word\par
of the Day" thesaurus.\par
As you can guess, probability is not the only metric.\par
Probability tends to select words\par
that are really, really common.\par
So if there's a word like "the" that\par
occurs in most of your documents or most of your topics\par
and most of your labels, it's probably\par
going to be highly ranked under every single label.\par
But this is not usually a good description of labels.\par
The opposite would be a metric called pointwise\par
mutual information.\par
So the idea behind this metric is that you get a word w,\par
and given that you've seen this word w,\par
how much does it tell you about label l?\par
So mathematically, we're going to call this PMI.\par
And this is between word w and label l.\par
This is equal to the log of the probability of word w and label\par
l divided by the probability of seeing\par
the word w times the probability of seeing label l.\par
And usually, this is log base 2.\par
So this sort of metric comes from information theory.\par
And it kind of expresses how much\par
surprise in bits that you would have\par
with this sort of information.\par
So pointwise mutual information tells\par
you how much information is there\par
in the joint probability of w and l\par
versus just submarginals, the probability of w\par
and the probability of l.\par
So we can use a few identities from probability.\par
And if I derive this out, this is\par
log base 2 of the probability of label l given word w\par
times the probability of word w divided\par
by the probability of word w times the probability of label\par
l.\par
And obviously, these w's go away.\par
You get the log base 2 of the probability of label\par
w at label l given word w divided\par
by the probability of label l.\par
So if we look at this, we are comparing the probability\par
of our labels under just the model\par
as it is against the probability of the labels given\par
that we've seen word w.\par
And since we're taking a ratio here,\par
this is particularly sensitive to small probabilities,\par
because if you go from 0.6 to 0.7, not really a big deal.\par
But if you go from 0.007 to 0.06,\par
well it's a really big deal, especially in log scale.\par
So what does pointwise mutual information do in practice?\par
Well, I'll start with our conditional definition\par
of pointwise mutual information.\par
And suppose that we have this word "brigand."\par
That's not a terribly common word, but I like it a lot.\par
And let's just say that it appears in our corpus once.\par
And given that it appears in our corpus exactly once,\par
it's going to be associated with exactly one label.\par
So let's say that "brigand" happens once for label 7.\par
So let's compute the pointwise mutual information\par
between "brigand" and label 1 and pointwise\par
mutual information between "brigand" and label 7.\par
So PMI, so this is the log.\par
And then we have the probability of label 7\par
given the word "brigand."\par
Well, "brigand" never occurs in label 7.\par
So that's going to have a probability of 0 divided\par
by whatever the probability is.\par
Let's say 0.1.\par
So this is log of 0.\par
And that's equal to negative infinity.\par
So we will never choose "brigand" as a representative\par
for a label where it doesn't occur.\par
But let's see what happens when it occurs in label 7.\par
So that is log base 2.\par
And what is the probability of label 7\par
given that I've seen "brigand?"\par
Well, it only occurs in label 7.\par
So if I see "brigand," I know that it is label 7.\par
So that probability is actually 1.\par
And then this is divided by the probability of label,\par
let's say, 0.1.\par
So in this case, we are ending up with log base 2 of 10.\par
So when I'm choosing words to associate with label 7,\par
any words that occur only in level 7\par
are going to have the same pointwise mutual information.\par
And we're going to always select those as our topic label\par
summaries.\par
Let's do another example.\par
Let's suppose that I have two labels-- label 1 and label 2--\par
and I have a document.\par
So if I have label 1 and label 2,\par
I've got a couple words here-- "the cat," "the hat."\par
And I'm going to go through and do counts of all these.\par
So let's say that this occurs one time in label 1\par
and one time in label 2.\par
This occurs one time in label 1, zero times in label 2.\par
And this occurs times in label 1 and one time in label 2.\par
So in my labels, I've got two words each-- "the cat"\par
in label 1, "the hat" in label 2.\par
So if I want to compute the PMI between "the" and label 1,\par
well, this is log of-- what is the probability\par
of label 1, given that I've seen word "the?"\par
So we're going to ignore all these bottom parts.\par
Just look at "the" row.\par
So there are two of them.\par
And it occurs with label 1 1/2 of the time divided\par
by the total probability I see label 1.\par
Well, I have four instances.\par
Two of them are with label 1.\par
Two of them are with label 2.\par
So that is going to have 1/2.\par
And that is log base 2 of 1, which is equal 0.\par
So I'm going to try this with the other words.\par
PMI of "cat" and label 1, so this is log.\par
If I see cat, that only occurs with label 1.\par
So that is 1 divided by the probability of label 1.\par
So that is log base 2 of 2.\par
And that's equal to 1.\par
And I could do the same thing with the third word-- "hat"\par
label 1, so that's log.\par
"Hat" doesn't occur in label 1, so probability\par
of seeing label 1 given "hat" is 0 divided by 1/2.\par
And that's log base 2 of 0, which is negative infinity.\par
So if we look at these computations,\par
it tells us a lot about which words are selected using PMI.\par
So all the words that only occur in a given topic--\par
so if I want to figure out a summary for label 1\par
here-- well, they're going to have the highest PMI.\par
So I'm going to select "cat."\par
And they're all going to have an equal value\par
PMI no matter how many times they occur.\par
So whether "cat" happens once, or I have another word that\par
only happens in label 1 and it happens 157 times,\par
they're going to have exactly the same pointwise mutual\par
information with label 1.\par
And likewise, in words that never occur in label 1\par
are pretty obviously not going to be a good summary.\par
So what happens is that PMI tends\par
to select these sorts of words.\par
But the words that only occur when\par
associated with a single label, well, guess what?\par
Those words don't tend to happen, occur much at all\par
in the entire corpus.\par
So oftentimes, you'll get words that happen once, twice,\par
maybe two or three times.\par
And they tend not to be great summaries.\par
End of transcript. Skip to the start.\par
Downloads and transcripts\par
Handouts\par
}
 